{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimise_so_2D_architectures.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNxi0X5KbjysAXMsEqgc+CE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waldnerf/yieldCNN/blob/main/optimise_so_2D_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scvdKqsbxtWl",
        "outputId": "4707e288-ef7d-4449-b10a-198914aa4e99"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.6.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.18)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (5.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.1)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->sqlalchemy>=1.1.0->optuna) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Whxvocqxgnb"
      },
      "source": [
        "import os, sys\n",
        "import argparse\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import optuna\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "random.seed(4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj57hOZjw-40",
        "outputId": "c03c0216-9ede-4f52-a736-60c58079cf9d"
      },
      "source": [
        "import os, sys\n",
        "git_dir = '/content/yieldCNN/'\n",
        "if not os.path.exists(git_dir):\n",
        "  !git clone https://github.com/waldnerf/yieldCNN\n",
        "else:\n",
        "  %cd /content/yieldCNN\n",
        "  !git pull\n",
        "  sys.path.insert(0, git_dir)\n",
        "\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yieldCNN\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mO7zDi8xiHB"
      },
      "source": [
        "from yieldCNN.deeplearning.architecture_complexity_2d import *\n",
        "from yieldCNN.outputfiles.plot import *\n",
        "from yieldCNN.outputfiles.save import *\n",
        "from yieldCNN.outputfiles.evaluation import *\n",
        "from yieldCNN.sits.readingsits2D import *\n",
        "import yieldCNN.mysrc.constants as cst"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MbYweU_0Vbk"
      },
      "source": [
        "## Optimisation function for a CNN with a single input\n",
        "\n",
        "*   Input 1 is a 2D convolutional neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjeuJnc0W3S"
      },
      "source": [
        "\n",
        "def objective_CNNw_SISO(trial):\n",
        "    # 2. Suggest values of the hyperparameters using a trial object.\n",
        "    nbunits_conv_ = trial.suggest_int('nbunits_conv', 10, 45, step=5)\n",
        "    kernel_size_ = trial.suggest_int('kernel_size', 2, 5)\n",
        "    strides_ = trial.suggest_int('strides', 2, 5)\n",
        "    pool_size_ = trial.suggest_int('pool_size', 1, 5)\n",
        "    dropout_rate_ = trial.suggest_float('dropout_rate', 0, 0.2, step=0.05)\n",
        "    nb_fc_ = trial.suggest_categorical('nb_fc', [1, 2])\n",
        "    funits_fc_ = trial.suggest_categorical('funits_fc', [1, 2, 3])\n",
        "    activation_ = trial.suggest_categorical('activation', ['relu', 'sigmoid'])\n",
        "\n",
        "    # Define output filenames\n",
        "    fn_fig_val = dir_tgt / f'{(out_model).split(\".h5\")[0]}' \\\n",
        "                           f'_res_{trial.number}_val_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                           f'{round(dropout_rate_ * 100)}_{nb_fc_}_{funits_fc_}_{activation_}.png'\n",
        "    fn_fig_test = dir_tgt / f\"{(out_model).split('.h5')[0]}\" \\\n",
        "                            f'_res_{trial.number}_test_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                            f'{round(dropout_rate_ * 100)}_{nb_fc_}_{funits_fc_}_{activation_}.png'\n",
        "    fn_cv_test = dir_tgt / f'{(out_model).split(\".h5\")[0]}' \\\n",
        "                           f'_res_{trial.number}_test_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                           f'{round(dropout_rate_ * 100)}_{nb_fc_}_{funits_fc_}_{activation_}.csv'\n",
        "    out_model_file = dir_tgt / f'{out_model.split(\".h5\")[0]}_{crop_n}.h5'\n",
        "\n",
        "    model = Archi_2DCNNw_SISO(Xt,\n",
        "                              region_ohe,\n",
        "                              nbunits_conv=nbunits_conv_,\n",
        "                              kernel_size=kernel_size_,\n",
        "                              strides=strides_,\n",
        "                              pool_size=pool_size_,\n",
        "                              dropout_rate=dropout_rate_,\n",
        "                              nb_fc=nb_fc_,\n",
        "                              funits_fc=funits_fc_,\n",
        "                              activation=activation_,\n",
        "                              verbose=False)\n",
        "    mses_val, r2s_val, mses_test, r2s_test = [], [], [], []\n",
        "    df_val, df_test, df_details = None, None, None\n",
        "    cv_i = 0\n",
        "    for test_i in np.unique(groups):\n",
        "        val_i = random.choice([x for x in np.unique(groups) if x != test_i])\n",
        "        train_i = [x for x in np.unique(groups) if x != val_i and x != test_i]\n",
        "\n",
        "        Xt_train, Xv_train, ohe_train, y_train = subset_data(Xt, Xv, region_ohe, y,\n",
        "                                                             [x in train_i for x in groups])\n",
        "        Xt_val, Xv_val, ohe_val, y_val = subset_data(Xt, Xv, region_ohe, y, groups == val_i)\n",
        "        Xt_test, Xv_test, ohe_test, y_test = subset_data(Xt, Xv, region_ohe, y, groups == test_i)\n",
        "\n",
        "        # ---- Normalizing the data per band\n",
        "        min_per_t, max_per_t, min_per_v, max_per_v, min_per_y, max_per_y = computingMinMax(Xt_train,\n",
        "                                                                                           Xv_train,\n",
        "                                                                                           train_i)\n",
        "        # Normalise training set\n",
        "        Xt_train = normalizingData(Xt_train, min_per_t, max_per_t)\n",
        "        # Normalise validation set\n",
        "        Xt_val = normalizingData(Xt_val, min_per_t, max_per_t)\n",
        "        # Normalise test set\n",
        "        Xt_test = normalizingData(Xt_test, min_per_t, max_per_t)\n",
        "\n",
        "        # Normalise ys\n",
        "        transformer_y = MinMaxScaler().fit(y_train[:, [crop_n]])\n",
        "        ys_train = transformer_y.transform(y_train[:, [crop_n]])\n",
        "        ys_val = transformer_y.transform(y_val[:, [crop_n]])\n",
        "        ys_test = transformer_y.transform(y_test[:, [crop_n]])\n",
        "\n",
        "        # We compile our model with a sampled learning rate.\n",
        "        model, y_val_preds = cv_Model_MISO(model, Xt_train, ys_train, Xt_val, ys_val,\n",
        "                                           out_model_file, n_epochs=n_epochs, batch_size=batch_size)\n",
        "        y_val_preds = transformer_y.inverse_transform(y_val_preds)\n",
        "        out_val = np.concatenate([y_val[:, [crop_n]], y_val_preds], axis=1)\n",
        "\n",
        "        y_test_preds = model.predict(x={'ts_input': Xt_test})\n",
        "        y_test_preds = transformer_y.inverse_transform(y_test_preds)\n",
        "        out_test = np.concatenate([y_test[:, [crop_n]], y_test_preds], axis=1)\n",
        "        out_details = np.expand_dims(region_id[groups == test_i].T, axis=1)\n",
        "        if df_val is None:\n",
        "            df_val = out_val\n",
        "            df_test = out_test\n",
        "            df_details = np.concatenate([out_details, (np.ones_like(out_details) * test_i)], axis=1)\n",
        "        else:\n",
        "            df_val = np.concatenate([df_val, out_val], axis=0)\n",
        "            df_test = np.concatenate([df_test, out_test], axis=0)\n",
        "            df_details = np.concatenate([df_details,\n",
        "                                         np.concatenate([out_details, (np.ones_like(out_details) * test_i)], axis=1)],\n",
        "                                        axis=0)\n",
        "\n",
        "        mse_val = mean_squared_error(y_val[:, [crop_n]], y_val_preds, squared=False, multioutput='raw_values')\n",
        "        r2_val = r2_score(y_val[:, [crop_n]], y_val_preds)\n",
        "        mses_val.append(mse_val)\n",
        "        r2s_val.append(r2_val)\n",
        "        mse_test = mean_squared_error(y_test[:, [crop_n]], y_test_preds, squared=False, multioutput='raw_values')\n",
        "        r2_test = r2_score(y_test[:, [crop_n]], y_test_preds)\n",
        "        mses_test.append(mse_test)\n",
        "        r2s_test.append(r2_test)\n",
        "\n",
        "        trial.report(np.mean(r2s_val), cv_i)  # report mse\n",
        "        if trial.should_prune():  # let optuna decide whether to prune\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "        cv_i += 1\n",
        "\n",
        "    av_rmse_val = np.mean(mses_val)\n",
        "    av_r2_val = np.mean(r2s_val)\n",
        "    av_rmse_test = np.mean(mses_test)\n",
        "\n",
        "    plt.plot([0, 5], [0, 5], '-', color='black')\n",
        "    plt.plot(df_val[:, 1], df_val[:, 0], '.')\n",
        "    plt.title(f'RMSE: {np.round(av_rmse_val, 4)} - R^2 = {np.round(np.mean(r2s_val), 4)}')\n",
        "\n",
        "    plt.xlabel('Predictions (t/ha)')\n",
        "    plt.ylabel('Observations (t/ha)')\n",
        "    plt.xlim(0.0, 5.0)\n",
        "    plt.ylim(0.0, 5.0)\n",
        "\n",
        "    plt.savefig(fn_fig_val)\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot([0, 5], [0, 5], '--', color='black')\n",
        "    plt.plot(df_test[:, 0], df_test[:, 1], '.', color='orange')\n",
        "    plt.title(f'RMSE: {np.round(av_rmse_test, 4)} - R^2 = {np.round(np.mean(r2s_test), 4)}')\n",
        "\n",
        "    plt.xlabel('Predictions (t/ha)')\n",
        "    plt.ylabel('Observations (t/ha)')\n",
        "    plt.xlim(0.0, 5.0)\n",
        "    plt.ylim(0.0, 5.0)\n",
        "\n",
        "    plt.savefig(fn_fig_test)\n",
        "    plt.close()\n",
        "    # Save CV results\n",
        "    df_out = np.concatenate([df_details, df_test], axis=1)\n",
        "    pd.DataFrame(df_out, columns=['ASAP1_ID', 'Year', 'Observed', 'Predicted']).to_csv(fn_cv_test, index=False)\n",
        "\n",
        "    return av_r2_val\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JP5R8G90bzW"
      },
      "source": [
        "## Optimisation function for a CNN with two inputs\n",
        "\n",
        "*   Input 1 is a 2D convolutional neural network\n",
        "*   Input 2 is a vector of one-hot-encoded features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6q4JRmu0bJ7"
      },
      "source": [
        "def objective_CNNw_MISO(trial):\n",
        "    # 2. Suggest values of the hyperparameters using a trial object.\n",
        "    nbunits_conv_ = trial.suggest_int('nbunits_conv', 10, 45, step=5)\n",
        "    kernel_size_ = trial.suggest_int('kernel_size', 2, 5)\n",
        "    strides_ = trial.suggest_int('strides', 2, 5)\n",
        "    pool_size_ = trial.suggest_int('pool_size', 1, 5)\n",
        "    dropout_rate_ = trial.suggest_float('dropout_rate', 0, 0.2, step=0.05)\n",
        "    v_fc_ = trial.suggest_categorical('v_fc', [0, 1])\n",
        "    nbunits_v_ = trial.suggest_int('nbunits_v', 10, 25, step=5)\n",
        "    nb_fc_ = trial.suggest_categorical('nb_fc', [1, 2])\n",
        "    funits_fc_ = trial.suggest_categorical('funits_fc', [1, 2, 3])\n",
        "    activation_ = trial.suggest_categorical('activation', ['relu', 'sigmoid'])\n",
        "\n",
        "    # Define output filenames\n",
        "    fn_fig_val = dir_tgt / f'{(out_model).split(\".h5\")[0]}' \\\n",
        "                           f'_res_{trial.number}_val_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                           f'{round(dropout_rate_ * 100)}_{v_fc_}_{nbunits_v_}_{nb_fc_}_{funits_fc_}_{activation_}.png'\n",
        "    fn_fig_test = dir_tgt / f\"{(out_model).split('.h5')[0]}\" \\\n",
        "                            f'_res_{trial.number}_test_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                            f'{round(dropout_rate_ * 100)}_{v_fc_}_{nbunits_v_}_{nb_fc_}_{funits_fc_}_{activation_}.png'\n",
        "    fn_cv_test = dir_tgt / f'{(out_model).split(\".h5\")[0]}' \\\n",
        "                           f'_res_{trial.number}_test_{nbunits_conv_}_{kernel_size_}_{strides_}_{pool_size_}_' \\\n",
        "                           f'{round(dropout_rate_ * 100)}_{v_fc_}_{nbunits_v_}_{nb_fc_}_{funits_fc_}_{activation_}.csv'\n",
        "    out_model_file = dir_tgt / f'{out_model.split(\".h5\")[0]}_{crop_n}.h5'\n",
        "\n",
        "    model = Archi_2DCNNw_MISO(Xt,\n",
        "                              region_ohe,\n",
        "                              nbunits_conv=nbunits_conv_,\n",
        "                              kernel_size=kernel_size_,\n",
        "                              strides=strides_,\n",
        "                              pool_size=pool_size_,\n",
        "                              dropout_rate=dropout_rate_,\n",
        "                              v_fc=v_fc_,\n",
        "                              nbunits_v=nbunits_v_,\n",
        "                              nb_fc=nb_fc_,\n",
        "                              funits_fc=funits_fc_,\n",
        "                              activation=activation_,\n",
        "                              verbose=False)\n",
        "    mses_val, r2s_val, mses_test, r2s_test = [], [], [], []\n",
        "    df_val, df_test, df_details = None, None, None\n",
        "    cv_i = 0\n",
        "    for test_i in np.unique(groups):\n",
        "        val_i = random.choice([x for x in np.unique(groups) if x != test_i])\n",
        "        train_i = [x for x in np.unique(groups) if x != val_i and x != test_i]\n",
        "\n",
        "        Xt_train, Xv_train, ohe_train, y_train = subset_data(Xt, Xv, region_ohe, y,\n",
        "                                                             [x in train_i for x in groups])\n",
        "        Xt_val, Xv_val, ohe_val, y_val = subset_data(Xt, Xv, region_ohe, y, groups == val_i)\n",
        "        Xt_test, Xv_test, ohe_test, y_test = subset_data(Xt, Xv, region_ohe, y, groups == test_i)\n",
        "\n",
        "        # ---- Normalizing the data per band\n",
        "        min_per_t, max_per_t, min_per_v, max_per_v, min_per_y, max_per_y = computingMinMax(Xt_train,\n",
        "                                                                                           Xv_train,\n",
        "                                                                                           train_i)\n",
        "        # Normalise training set\n",
        "        Xt_train = normalizingData(Xt_train, min_per_t, max_per_t)\n",
        "        Xv_train = normalizingData(Xv_train, min_per_v, max_per_v)\n",
        "        # Normalise validation set\n",
        "        Xt_val = normalizingData(Xt_val, min_per_t, max_per_t)\n",
        "        Xv_val = normalizingData(Xv_val, min_per_v, max_per_v)\n",
        "        # Normalise test set\n",
        "        Xt_test = normalizingData(Xt_test, min_per_t, max_per_t)\n",
        "        Xv_test = normalizingData(Xv_test, min_per_v, max_per_v)\n",
        "\n",
        "        # Normalise ys\n",
        "        transformer_y = MinMaxScaler().fit(y_train[:, [crop_n]])\n",
        "        ys_train = transformer_y.transform(y_train[:, [crop_n]])\n",
        "        ys_val = transformer_y.transform(y_val[:, [crop_n]])\n",
        "        ys_test = transformer_y.transform(y_test[:, [crop_n]])\n",
        "\n",
        "        # ---- concatenate OHE and Xv / Here we discard the proportion of each crop and only keep OHE\n",
        "        Xv_train = ohe_train  # np.concatenate([Xv_train[:, [crop_n]], ohe_train], axis=1)\n",
        "        Xv_val = ohe_val  # np.concatenate([Xv_val[:, [crop_n]], ohe_val], axis=1)\n",
        "        Xv_test = ohe_test  #np.concatenate([Xv_test[:, [crop_n]], ohe_test], axis=1)\n",
        "\n",
        "        # We compile our model with a sampled learning rate.\n",
        "        model, y_val_preds = cv_Model_MISO(model, Xt_train, Xv_train, ys_train, Xt_val, Xv_val, ys_val,\n",
        "                                           out_model_file, n_epochs=n_epochs, batch_size=batch_size)\n",
        "        y_val_preds = transformer_y.inverse_transform(y_val_preds)\n",
        "        out_val = np.concatenate([y_val[:, [crop_n]], y_val_preds], axis=1)\n",
        "\n",
        "        y_test_preds = model.predict(x={'ts_input': Xt_test, 'v_input': Xv_test})\n",
        "        y_test_preds = transformer_y.inverse_transform(y_test_preds)\n",
        "        out_test = np.concatenate([y_test[:, [crop_n]], y_test_preds], axis=1)\n",
        "        out_details = np.expand_dims(region_id[groups == test_i].T, axis=1)\n",
        "        if df_val is None:\n",
        "            df_val = out_val\n",
        "            df_test = out_test\n",
        "            df_details = np.concatenate([out_details, (np.ones_like(out_details) * test_i)], axis=1)\n",
        "        else:\n",
        "            df_val = np.concatenate([df_val, out_val], axis=0)\n",
        "            df_test = np.concatenate([df_test, out_test], axis=0)\n",
        "            df_details = np.concatenate([df_details,\n",
        "                                         np.concatenate([out_details, (np.ones_like(out_details) * test_i)], axis=1)],\n",
        "                                        axis=0)\n",
        "\n",
        "        mse_val = mean_squared_error(y_val[:, [crop_n]], y_val_preds, squared=False, multioutput='raw_values')\n",
        "        r2_val = r2_score(y_val[:, [crop_n]], y_val_preds)\n",
        "        mses_val.append(mse_val)\n",
        "        r2s_val.append(r2_val)\n",
        "        mse_test = mean_squared_error(y_test[:, [crop_n]], y_test_preds, squared=False, multioutput='raw_values')\n",
        "        r2_test = r2_score(y_test[:, [crop_n]], y_test_preds)\n",
        "        mses_test.append(mse_test)\n",
        "        r2s_test.append(r2_test)\n",
        "\n",
        "        trial.report(np.mean(r2s_val), cv_i)  # report mse\n",
        "        if trial.should_prune():  # let optuna decide whether to prune\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "        cv_i += 1\n",
        "\n",
        "    av_rmse_val = np.mean(mses_val)\n",
        "    av_r2_val = np.mean(r2s_val)\n",
        "    av_rmse_test = np.mean(mses_test)\n",
        "\n",
        "    plt.plot([0, 5], [0, 5], '-', color='black')\n",
        "    plt.plot(df_val[:, 1], df_val[:, 0], '.')\n",
        "    plt.title(f'RMSE: {np.round(av_rmse_val, 4)} - R^2 = {np.round(np.mean(r2s_val), 4)}')\n",
        "\n",
        "    plt.xlabel('Predictions (t/ha)')\n",
        "    plt.ylabel('Observations (t/ha)')\n",
        "    plt.xlim(0.0, 5.0)\n",
        "    plt.ylim(0.0, 5.0)\n",
        "\n",
        "    plt.savefig(fn_fig_val)\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot([0, 5], [0, 5], '--', color='black')\n",
        "    plt.plot(df_test[:, 0], df_test[:, 1], '.', color='orange')\n",
        "    plt.title(f'RMSE: {np.round(av_rmse_test, 4)} - R^2 = {np.round(np.mean(r2s_test), 4)}')\n",
        "\n",
        "    plt.xlabel('Predictions (t/ha)')\n",
        "    plt.ylabel('Observations (t/ha)')\n",
        "    plt.xlim(0.0, 5.0)\n",
        "    plt.ylim(0.0, 5.0)\n",
        "\n",
        "    plt.savefig(fn_fig_test)\n",
        "    plt.close()\n",
        "    # Save CV results\n",
        "    df_out = np.concatenate([df_details, df_test], axis=1)\n",
        "    pd.DataFrame(df_out, columns=['ASAP1_ID', 'Year', 'Observed', 'Predicted']).to_csv(fn_cv_test, index=False)\n",
        "\n",
        "    return av_r2_val\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6sCGZCq0pEe"
      },
      "source": [
        "## Optimise 2D CNN architecture for all time steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGOlsnVu0r21",
        "outputId": "100647eb-cb21-46d6-db7a-0a7b87c97f59"
      },
      "source": [
        "# ---- Parameters to set\n",
        "fn_indata = cst.my_project.data_dir / f'{cst.target}_full_2d_dataset.pickle'\n",
        "dir_out = cst.my_project.params_dir\n",
        "model_type='2DCNNw_MISO'\n",
        "overwrite=True\n",
        "    \n",
        "n_channels = 4  # -- NDVI, Rad, Rain, Temp\n",
        "\n",
        "# ---- Get filenames\n",
        "print(\"Input file: \", os.path.basename(str(fn_indata)))\n",
        "\n",
        "# ---- output files\n",
        "dir_out.mkdir(parents=True, exist_ok=True)\n",
        "dir_res = dir_out / f'Archi+{str(model_type)}'\n",
        "dir_res.mkdir(parents=True, exist_ok=True)\n",
        "print(\"noarchi: \", model_type)\n",
        "out_model = f'archi-{model_type}.h5'\n",
        "\n",
        "# ---- Downloading\n",
        "Xt_full, Xv, region_id, groups, y = data_reader(fn_indata)\n",
        "\n",
        "# ---- Convert region to one hot\n",
        "region_ohe = add_one_hot(region_id)\n",
        "\n",
        "# ---- Getting train/val/test data\n",
        "\n",
        "# ---- variables\n",
        "n_epochs = 70\n",
        "batch_size = 800\n",
        "n_trials = 100\n",
        "\n",
        "# loop through all crops\n",
        "for crop_n in range(y.shape[1]):\n",
        "    dir_crop = dir_res / f'crop_{crop_n}'\n",
        "    dir_crop.mkdir(parents=True, exist_ok=True)\n",
        "    # loop by month\n",
        "    for month in range(2, 9):\n",
        "        dir_tgt = dir_crop / f'month_{month}'\n",
        "        dir_tgt.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if (len([x for x in dir_tgt.glob('best_model')]) != 0) & (overwrite is False):\n",
        "            pass\n",
        "        else:\n",
        "            rm_tree(dir_tgt)\n",
        "            idx = (month + 1) * 3\n",
        "            Xt = Xt_full[:, :, 0:idx, :]\n",
        "\n",
        "            study = optuna.create_study(direction='maximize',\n",
        "                                        pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=8)\n",
        "                                        )\n",
        "            if model_type == '2DCNNw_SISO':\n",
        "                study.optimize(objective_CNNw_SISO, n_trials=n_trials)\n",
        "            if model_type == '2DCNNw_MISO':\n",
        "                study.optimize(objective_CNNw_MISO, n_trials=n_trials)\n",
        "\n",
        "            trial = study.best_trial\n",
        "            print('------------------------------------------------')\n",
        "            print('--------------- Optimisation results -----------')\n",
        "            print('------------------------------------------------')\n",
        "            print(\"Number of finished trials: \", len(study.trials))\n",
        "            print(f\"\\n           Best trial ({trial.number})        \\n\")\n",
        "            print(\"R2: \", trial.value)\n",
        "            print(\"Params: \")\n",
        "            for key, value in trial.params.items():\n",
        "                print(\"{}: {}\".format(key, value))\n",
        "\n",
        "            joblib.dump(study, os.path.join(dir_tgt, f'study_{crop_n}_{model_type}.dump'))\n",
        "            # dumped_study = joblib.load(os.path.join(cst.my_project.meta_dir, 'study_in_memory_storage.dump'))\n",
        "            # dumped_study.trials_dataframe()\n",
        "            df = study.trials_dataframe().to_csv(os.path.join(dir_tgt, f'study_{crop_n}_{model_type}.csv'))\n",
        "            # fig = optuna.visualization.plot_slice(study)\n",
        "            print('------------------------------------------------')\n",
        "\n",
        "            save_best_model(dir_tgt, f'res_{trial.number}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-07-02 10:49:11,976]\u001b[0m A new study created in memory with name: no-name-2ffef8c6-518f-409a-8312-d76bd6af8f64\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input file:  Algeria_full_2d_dataset.pickle\n",
            "noarchi:  2DCNNw_MISO\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fab62358f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fab63364680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning:\n",
            "\n",
            "The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}